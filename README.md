# Concert Ticket Booking System - Cloud Native Travel Application

**Online Exam Cloud Services**  
**Topik: Concerts Event Ticketing System**

## ğŸ“‹ Executive Summary

Sistem pemesanan tiket konser berbasis cloud-native yang dirancang untuk menangani beban tinggi dengan observability penuh. Proyek ini mendemonstrasikan:
- âœ… Arsitektur cloud-native dengan containerization
- âœ… Implementasi monitoring & observability (Prometheus + Grafana)
- âœ… Load testing dengan simulasi beban realistis (k6)
- âœ… Analisis performa dan bottleneck identification
- âœ… Incident response dan improvement planning

---

## ğŸ¯ 1. ARSITEKTUR SISTEM CLOUD-NATIVE

### 1.1 Komponen Arsitektur

#### **A. Frontend Layer**
- **Teknologi**: Nginx Alpine (Static Web Server)
- **Fungsi**: 
  - Menyajikan interface HTML/CSS/JavaScript (Bootstrap 5)
  - Reverse proxy ke backend API
  - Static asset serving
- **Port**: 80
- **Resource**: 
  - Memory: ~10MB
  - CPU: Minimal (static content)

#### **B. Backend API Layer**
- **Teknologi**: FastAPI (Python 3.11) + Uvicorn
- **Fungsi**:
  - RESTful API endpoints untuk booking
  - Business logic processing
  - Database connection pooling
  - Metrics instrumentation
- **Endpoints**:
  - `POST /setup` - Initialize event
  - `POST /book` - Book ticket (naive implementation)
  - `GET /status` - Get ticket availability
  - `GET /health` - Health check
  - `GET /metrics` - Prometheus metrics
- **Port**: 8000
- **Resource Allocation**:
  - Memory: 70-85MB under load
  - CPU: 0-10% per core
  - Workers: 1 (single process for demo)

#### **C. Database Layer**
- **Teknologi**: MariaDB 11.2
- **Fungsi**:
  - Persistent storage untuk events & bookings
  - Transaction handling
  - ACID compliance
- **Schema**:
  ```sql
  events: id, name, total_tickets, available_tickets
  bookings: id, user_id, event_id, timestamp
  ```
- **Port**: 3306
- **Resource**: 
  - Memory: ~400MB
  - Storage: Volume-mounted

#### **D. Monitoring & Observability Stack**

**Prometheus (Metrics Collection)**
- **Teknologi**: Prometheus 2.x
- **Fungsi**:
  - Scraping metrics dari backend setiap 5 detik
  - Time-series database
  - Query engine (PromQL)
- **Port**: 9090
- **Retention**: 15 days
- **Storage**: ~100MB per day

**Grafana (Visualization)**
- **Teknologi**: Grafana OSS
- **Fungsi**:
  - Dashboard visualization
  - Alerting (future)
  - Query builder
- **Port**: 3000
- **Credentials**: admin/admin
- **Dashboards**: Pre-configured 7-panel dashboard

#### **E. Load Testing**
- **Teknologi**: k6 (Grafana)
- **Fungsi**:
  - Concurrent user simulation
  - Performance testing
  - Metrics collection
- **Execution**: On-demand via Docker

### 1.2 Diagram Arsitektur Lengkap

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      DOCKER NETWORK (ticket-network)            â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚   Frontend   â”‚   HTTP  â”‚    Backend      â”‚                   â”‚
â”‚  â”‚   (Nginx)    â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”‚   (FastAPI)     â”‚                   â”‚
â”‚  â”‚   :80        â”‚         â”‚   :8000         â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚         â”‚                          â”‚                            â”‚
â”‚         â”‚                          â”‚ SQL                        â”‚
â”‚         â”‚                          â–¼                            â”‚
â”‚         â”‚                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚         â”‚                  â”‚    MariaDB      â”‚                  â”‚
â”‚         â”‚                  â”‚    :3306        â”‚                  â”‚
â”‚         â”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚         â”‚                          â–²                            â”‚
â”‚         â”‚                          â”‚                            â”‚
â”‚         â”‚                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚         â”‚                  â”‚ Volume: ticket-db                  â”‚
â”‚         â”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚         â”‚                                                       â”‚
â”‚         â”‚                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚         â”‚                  â”‚   Prometheus    â”‚                  â”‚
â”‚         â”‚           scrape â”‚   :9090         â”‚                  â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  (Metrics)      â”‚                  â”‚
â”‚              /metrics       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                                     â”‚                           â”‚
â”‚                                     â”‚ PromQL                    â”‚
â”‚                                     â–¼                           â”‚
â”‚                             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚                             â”‚    Grafana      â”‚                 â”‚
â”‚                             â”‚    :3000        â”‚                 â”‚
â”‚                             â”‚  (Dashboards)   â”‚                 â”‚
â”‚                             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                               â”‚
â”‚  â”‚      k6      â”‚  Load Test                                    â”‚
â”‚  â”‚  (on-demand) â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Backend :8000                     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

External Access:
  http://localhost:80      â†’ Frontend UI
  http://localhost:8000    â†’ Backend API
  http://localhost:9090    â†’ Prometheus UI
  http://localhost:3000    â†’ Grafana Dashboard
```

### 1.3 Komunikasi Antar Komponen

#### **A. Frontend â†” Backend**
- **Protokol**: HTTP/1.1 REST API
- **Format**: JSON
- **Flow**:
  1. User mengakses http://localhost
  2. Nginx serve static HTML
  3. JavaScript melakukan AJAX call ke `http://localhost:8000/book`
  4. Backend memproses dan return JSON response
- **Error Handling**: Status codes (200, 400, 404, 500)

#### **B. Backend â†” Database**
- **Protokol**: MySQL Wire Protocol
- **Driver**: `mysql-connector-python`
- **Connection**:
  - Host: `mariadb` (Docker DNS)
  - Port: 3306
  - Database: `ticket_booking`
- **Connection Pooling**: Disabled (new connection per request)
- **Transaction**: Auto-commit enabled
- **Instrumentation**: Query duration tracking per operation

#### **C. Prometheus â†” Backend**
- **Protokol**: HTTP Pull (Scraping)
- **Endpoint**: `http://backend:8000/metrics`
- **Interval**: 5 seconds
- **Format**: Prometheus text exposition format
- **Metrics Library**: 
  - `prometheus-fastapi-instrumentator` (automatic)
  - `prometheus-client` (custom metrics)

#### **D. Grafana â†” Prometheus**
- **Protokol**: HTTP (PromQL queries)
- **Datasource**: `http://prometheus:9090`
- **Refresh**: 5s auto-refresh
- **Query Examples**:
  - RPS: `rate(http_requests_total[1m])`
  - P95 Latency: `histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[1m]))`

### 1.4 Deployment Strategy (Containerization & Orchestration)

#### **A. Containerization**
- **Tool**: Docker & Docker Compose
- **Container Images**:
  1. **Backend**: `python:3.11-slim`
     - Build: Multi-stage untuk size optimization
     - Dependencies: requirements.txt
  2. **Frontend**: `nginx:alpine`
     - Size: ~20MB
  3. **Database**: `mariadb:11.2`
  4. **Prometheus**: `prom/prometheus:latest`
  5. **Grafana**: `grafana/grafana:latest`
  6. **k6**: `grafana/k6:latest`

#### **B. Orchestration dengan Docker Compose**

**docker-compose.yml Structure**:
```yaml
services:
  mariadb:      # Database layer
  backend:      # API layer (depends on mariadb)
  frontend:     # Web layer (depends on backend)
  prometheus:   # Monitoring (scrapes backend)
  grafana:      # Visualization (queries prometheus)
  k6:           # Load testing (targets backend)

networks:
  ticket-network:  # Isolated network

volumes:
  ticket-db:       # Persistent database storage
```

**Deployment Steps**:
```bash
# 1. Build dan start semua services
docker-compose up -d --build

# 2. Verifikasi health
docker-compose ps

# 3. Initialize database & event
curl -X POST http://localhost:8000/setup \
  -H "Content-Type: application/json" \
  -d '{"event_name":"Concert Coldplay","total_tickets":1000}'

# 4. Run load test
docker exec ticket-k6 k6 run --vus 100 --duration 30s /scripts/load-test.js

# 5. Access monitoring
# Grafana: http://localhost:3000/d/ticket-booking-metrics
```

#### **C. Dependency Management**
```
mariadb (base)
   â†“
backend (depends_on: mariadb)
   â†“
frontend (depends_on: backend)
   â†“
prometheus (scrapes backend)
   â†“
grafana (queries prometheus)
```

**Health Checks**:
- Backend: HTTP GET `/health` every 10s
- Database: `mysqladmin ping` every 10s
- Frontend: TCP port 80 check

#### **D. Resource Limits** (Production-ready)
```yaml
services:
  backend:
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 512M
        reservations:
          cpus: '0.5'
          memory: 256M
```

### 1.5 Environment Configuration

**Backend (.env)**:
```bash
DB_HOST=mariadb
DB_PORT=3306
DB_USER=ticket_user
DB_PASSWORD=ticket_pass
DB_NAME=ticket_booking
```

**Secrets Management**: Environment variables (Docker secrets untuk production)

---

## ğŸ“Š 2. METRIK OBSERVABILITY (8 Metrik Penting)

---

## ğŸ“Š 2. METRIK OBSERVABILITY (8 Metrik Penting)

### 2.1 Tabel Metrik & Justifikasi

| # | Metrik | Alasan Pemilihan | Kondisi Sehat | Kondisi Bermasalah | Panel Grafana |
|---|--------|------------------|---------------|---------------------|---------------|
| **1** | **Request Per Second (RPS)** | Mengukur throughput sistem - indikator utama untuk perencanaan kapasitas dan analisis pola traffic | 50-500 RPS | >1000 RPS (kelebihan beban) atau <10 RPS (kurang dimanfaatkan) | "Request Rate" - Query: `sum(rate(http_requests_total[1m]))` |
| **2** | **P95 Response Time** | Menangkap latency terburuk untuk 95% pengguna - lebih representatif dari rata-rata untuk pengalaman pengguna | <500ms | >2s (UX buruk), >5s (risiko timeout) | "P95 Latency" - Query: `histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[1m]))` |
| **3** | **Error Rate (4xx/5xx)** | Kritis untuk reliabilitas - langsung berdampak pada tingkat keberhasilan pengguna dan pendapatan | <1% (0.01) | >5% (insiden besar), >10% (kritis) | "Error Rate" - Query: `sum(rate(http_requests_total{status=~"4..|5.."}[1m])) / sum(rate(http_requests_total[1m]))` |
| **4** | **Backend CPU Usage** | Indikator pemanfaatan sumber daya - untuk perencanaan kapasitas dan identifikasi bottleneck. Diukur via background thread menggunakan `cpu_times()` untuk metrik container yang akurat | 20-70% | >90% (saturasi), <5% (kurang dimanfaatkan) | "Backend CPU %" - Gauge metric `backend_cpu_usage_percent` |
| **5** | **Database Query Latency** | Mengidentifikasi database sebagai sumber bottleneck - sering menjadi penyebab utama response time tinggi | p95 <50ms | p95 >200ms (query lambat), >500ms (masalah serius) | "DB Latency" - Query: `histogram_quantile(0.95, rate(db_query_duration_seconds_bucket[1m]))` |
| **6** | **Concurrent Requests** | Mengukur beban sistem secara real-time - membantu memahami bottleneck konkurensi | <100 | >300 (penumpukan antrian), >500 (sistem kelebihan beban) | "Requests In Progress" - Gauge `http_requests_inprogress` |
| **7** | **Tickets Remaining** | Metrik bisnis - langsung menunjukkan kesehatan inventori dan tingkat keberhasilan booking | >0 tiket | 0 tiket (habis terjual), negatif (bug overselling) | "Tickets Remaining" - Gauge `tickets_remaining` |
| **8** | **Memory Usage** | Deteksi kebocoran memori - mencegah OOM kills dan crash sistem | 50-80% (70MB) | >90% (tekanan memori), tren naik (kebocoran) | "Memory Usage" - Gauge `backend_memory_usage_bytes` |

### 2.2 Detail Metrik

#### **Metrik 1: Request Per Second (RPS)**
- **Sumber**: `prometheus-fastapi-instrumentator`
- **Metric Name**: `http_requests_total` (Counter)
- **Query**: `sum(rate(http_requests_total[1m]))`
- **Justifikasi Detail**:
  - **Golden Signal**: "Traffic" dari Google's 4 Golden Signals
  - Baseline untuk semua analisis performa
  - Korelasi dengan business KPI (bookings per minute)
  - Early warning untuk traffic spikes
- **Threshold Alert**:
  - Warning: >800 RPS
  - Critical: >1000 RPS

#### **Metrik 2: P95 Response Time**
- **Sumber**: `prometheus-fastapi-instrumentator`
- **Metric Name**: `http_request_duration_seconds_bucket` (Histogram)
- **Query**: `histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[1m]))`
- **Justifikasi Detail**:
  - P95 lebih baik dari average karena tidak terpengaruh outliers
  - SLA standard industry: p95 < 500ms
  - User experience: >1s terasa lambat, >3s bounce rate tinggi
  - Menangkap tail latency yang penting untuk 5% worst users
- **Threshold Alert**:
  - Warning: >1s
  - Critical: >3s

#### **Metrik 3: Error Rate (4xx/5xx)**
- **Sumber**: `prometheus-fastapi-instrumentator`
- **Metric Name**: `http_requests_total{status=~"4..|5.."}` (Counter)
- **Query**: `sum(rate(http_requests_total{status=~"4..|5.."}[1m])) / sum(rate(http_requests_total[1m]))`
- **Justifikasi Detail**:
  - Direct impact pada revenue (failed bookings = lost sales)
  - 4xx: Client errors (sold out, invalid input)
  - 5xx: Server errors (bugs, database issues)
  - Industry standard: <0.1% untuk production
- **Threshold Alert**:
  - Warning: >1%
  - Critical: >5%

#### **Metrik 4: Backend CPU Usage**
- **Sumber**: Custom metric via `psutil` background thread
- **Nama Metrik**: `backend_cpu_usage_percent` (Gauge)
- **Query**: Nilai gauge langsung
- **Detail Implementasi**:
  - **Background Thread**: Thread khusus menghitung CPU menggunakan `cpu_times()`
  - **Formula**: `(user_time + system_time) / elapsed_time * 100`
  - **Sampling Rate**: Setiap 500ms untuk pengukuran akurat
  - **Mengapa cpu_times()**: `cpu_percent()` mengembalikan 0 di container; `cpu_times()` melacak konsumsi CPU proses yang sebenarnya
- **Pola Kode**:
  ```python
  def cpu_monitor_thread():
      prev_times = process.cpu_times()
      prev_time = time.time()
      while True:
          time.sleep(0.5)
          curr_times = process.cpu_times()
          elapsed = time.time() - prev_time
          cpu_used = (curr_times.user - prev_times.user) + 
                     (curr_times.system - prev_times.system)
          cpu_percent = (cpu_used / elapsed) * 100
          # Updates gauge metric
  ```
- **Justifikasi Detail**:
  - Mengidentifikasi bottleneck komputasi
  - Pemicu horizontal scaling
  - CPU >90% â†’ antrian request â†’ peningkatan latency
  - Korelasi dengan RPS untuk perencanaan kapasitas
  - Akurat di lingkungan container (Docker)
- **Threshold Alert**:
  - Peringatan: >80%
  - Kritis: >95%
- **Nilai Teramati**:
  - Idle: 0-2%
  - Saat beban (300 VUs): 75-86%

#### **Metrik 5: Database Query Latency**
- **Sumber**: Custom metric via timing instrumentation
- **Metric Name**: `db_query_duration_seconds_bucket` (Histogram)
- **Labels**: `operation={select|insert|update|delete}`, `table={events|bookings}`
- **Query**: `histogram_quantile(0.95, rate(db_query_duration_seconds_bucket{operation="select"}[1m]))`
- **Justifikasi Detail**:
  - Database often bottleneck dalam transactional systems
  - Breakdown per operation type untuk root cause analysis
  - Identifies missing indexes, lock contention
  - Early warning untuk database scaling needs
- **Threshold Alert**:
  - Warning: p95 >100ms
  - Critical: p95 >500ms

#### **Metrik 6: Concurrent Requests**
- **Sumber**: `prometheus-fastapi-instrumentator`
- **Metric Name**: `http_requests_inprogress` (Gauge)
- **Query**: `http_requests_inprogress{handler="/book"}`
- **Justifikasi Detail**:
  - Real-time load indicator
  - Queue depth measurement
  - Predicts saturation before it happens
  - Correlate dengan CPU untuk worker sizing
- **Threshold Alert**:
  - Warning: >200 concurrent
  - Critical: >400 concurrent

#### **Metrik 7: Tickets Remaining (Sisa Tiket)**
- **Sumber**: Custom business metric
- **Nama Metrik**: `tickets_remaining` (Gauge)
- **Label**: `event_id` - mendukung multiple event secara dinamis
- **Query**: `tickets_remaining{event_id="1"}` atau `sum(tickets_remaining)` untuk semua event
- **Nilai Saat Ini** (dari endpoint `/metrics`):
  ```
  tickets_remaining{event_id="1"} 56.0
  tickets_remaining{event_id="2"} 100.0
  ...
  tickets_remaining{event_id="9"} 88.0
  ```
- **Catatan Implementasi**: Frontend mengambil `event_id` saat ini secara dinamis dari endpoint `/status`
- **Justifikasi Detail**:
  - Metrik kritis bisnis (inventori)
  - Mendeteksi bug overselling (nilai negatif)
  - Tingkat keberhasilan booking = f(tickets_remaining)
  - Insight marketing (prediksi sold out)
- **Threshold Alert**:
  - Peringatan: <10 tiket tersisa
  - Kritis: <0 (overselling terdeteksi!)

#### **Metrik 8: Memory Usage**
- **Sumber**: Custom metric via `psutil`
- **Metric Name**: `backend_memory_usage_bytes` (Gauge)
- **Query**: `backend_memory_usage_bytes / 1024 / 1024` (convert to MB)
- **Justifikasi Detail**:
  - Memory leak detection
  - Prevents OOM kills
  - Resource reservation planning
  - Correlation dengan request rate (connection pooling issues)
- **Threshold Alert**:
  - Warning: >400MB
  - Critical: >500MB (approaching limit)
- **Current Value**: ~78MB (`backend_memory_usage_bytes 7.872512e+07`)

#### **Metrik Tambahan: Booking Attempts (Percobaan Booking)**
- **Sumber**: Custom metric
- **Nama Metrik**: `booking_attempts_total` (Counter)
- **Label**: `status={success|failed|sold_out}`
- **Query**: `sum(rate(booking_attempts_total{status="success"}[1m])) / sum(rate(booking_attempts_total[1m]))`
- **Nilai Saat Ini** (dari endpoint `/metrics`):
  ```
  booking_attempts_total{status="failed"} 79777.0
  booking_attempts_total{status="success"} 50.0
  ```
- **Justifikasi Detail**:
  - Indikator tingkat keberhasilan bisnis
  - Mengidentifikasi kegagalan booking vs event sold out
  - Analisis dampak pendapatan
  - Deteksi race condition (tingkat gagal tinggi)

### 2.3 Prometheus Query Examples

```promql
# 1. RPS per endpoint
sum by(handler) (rate(http_requests_total[1m]))

# 2. P95 latency per endpoint
histogram_quantile(0.95, sum by(handler, le) (rate(http_request_duration_seconds_bucket[1m])))

# 3. Error rate trend (5min)
sum(rate(http_requests_total{status=~"5.."}[5m])) / sum(rate(http_requests_total[5m]))

# 4. Database operations breakdown
sum by(operation, table) (rate(db_query_duration_seconds_count[1m]))

# 5. Booking success rate
sum(rate(booking_attempts_total{status="success"}[1m])) / sum(rate(booking_attempts_total[1m]))
```

---

## ğŸ”¬ 3. SIMULASI BEBAN (LOAD TESTING)

### 3.1 Setup Event untuk Testing

```bash
# Initialize event dengan 5000 tickets
curl -X POST http://localhost:8000/setup \
  -H "Content-Type: application/json" \
  -d '{
    "event_name": "Concert Metallica Jakarta",
    "total_tickets": 5000
  }'
```

### 3.2 Skenario 1: Beban Normal

#### **A. Deskripsi Skenario**
Simulasi traffic normal pada hari biasa (non-peak hours) dengan user organik yang melakukan booking secara bertahap.

#### **B. Perintah Simulasi**
```bash
docker exec ticket-k6 k6 run \
  --vus 50 \
  --duration 60s \
  /scripts/load-test.js
```

#### **C. Parameter Skenario**
| Parameter | Nilai | Penjelasan |
|-----------|-------|------------|
| **Virtual Users (VUs)** | 50 | Concurrent users aktif |
| **Duration** | 60 seconds | Total waktu test |
| **Ramp-up** | 10s | Gradual increase ke 50 users |
| **Request Rate** | ~50-100 RPS | Expected throughput |
| **Think Time** | 100ms | Delay antar request per user |

#### **D. Endpoints yang Diuji**
- **Primary**: `POST /book` (80% traffic)
- **Secondary**: `GET /status` (15% traffic)
- **Health**: `GET /health` (5% traffic)

#### **E. k6 Script Configuration**
```javascript
export const options = {
  stages: [
    { duration: '10s', target: 50 },   // Ramp up ke 50 users
    { duration: '40s', target: 50 },   // Hold 50 users  
    { duration: '10s', target: 0 },    // Ramp down
  ],
  thresholds: {
    http_req_duration: ['p(95)<500'], // 95% requests < 500ms
    http_req_failed: ['rate<0.01'],   // Error rate < 1%
  },
};
```

### 3.3 Skenario 2: Beban Tinggi

#### **A. Deskripsi Skenario**
Simulasi flash sale atau presale tiket konser populer dengan traffic spike mendadak (Black Friday scenario).

#### **B. Perintah Simulasi**
```bash
docker exec ticket-k6 k6 run \
  --vus 300 \
  --duration 30s \
  /scripts/load-test.js
```

#### **C. Parameter Skenario**
| Parameter | Nilai | Penjelasan |
|-----------|-------|------------|
| **Virtual Users (VUs)** | 300 | Peak concurrent users |
| **Duration** | 30 seconds | Short burst test |
| **Ramp-up** | Instant | Immediate load (flash sale) |
| **Request Rate** | ~300-600 RPS | High throughput |
| **Think Time** | 100ms | Aggressive booking attempts |

#### **D. Endpoints yang Diuji**
- **Primary**: `POST /book` (95% traffic - aggressive booking)
- **Secondary**: `GET /status` (5% traffic)

#### **E. k6 Script Configuration**
```javascript
export const options = {
  stages: [
    { duration: '30s', target: 300 },  // Immediate spike to 300 users
  ],
  thresholds: {
    http_req_duration: ['p(95)<2000'], // Relaxed: 95% < 2s
    http_req_failed: ['rate<0.10'],    // Allow 10% errors (sold out)
  },
};
```

---

## ğŸ“ˆ 4. HASIL SIMULASI & ANALISIS

### 4.1 Tabel Hasil Simulasi

| Metrik | Beban Normal (50 VUs) | Beban Tinggi (300 VUs) | Sumber Panel | Ambang Batas |
|--------|------------------------|-------------------------|--------------|--------------|
| **Request Per Second (RPS)** | 67 RPS | 361 RPS | Grafana: "Request Rate" panel<br/>Query: `sum(rate(http_requests_total[1m]))` | Normal: <500 RPS<br/>High: <1000 RPS |
| **P95 Response Time** | 480ms | 23.8s | Grafana: "P95 Latency" panel<br/>Query: `histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[1m]))` | Normal: <500ms<br/>High: <3s |
| **Error Rate (4xx/5xx)** | 0% | 45.4% (400 errors) | Grafana: "Error Rate" panel<br/>Query: `sum(rate(http_requests_total{status=~"4..|5.."}[1m])) / sum(rate(http_requests_total[1m]))` | Normal: <1%<br/>High: <10% |
| **Backend CPU Usage** | 2-5% | 75-86% | Grafana: "Backend CPU" gauge<br/>Metrik: `backend_cpu_usage_percent`<br/>Implementasi: Background thread menggunakan `cpu_times()` | Normal: <70%<br/>Tinggi: <90% |
| **Database Latency (p95)** | 45ms | 150ms | Grafana: "DB Latency" panel<br/>Query: `histogram_quantile(0.95, rate(db_query_duration_seconds_bucket{operation="select"}[1m]))` | Normal: <100ms<br/>High: <500ms |
| **Concurrent Requests** | 10-20 | 300 (peak) | Prometheus: `http_requests_inprogress{handler="/book"}` | Normal: <100<br/>High: <400 |
| **Tickets Remaining** | 2,786 (sold 2,214) | 0 (sold out 5,000) | Grafana: "Tickets Remaining" gauge<br/>Metric: `tickets_remaining` | >0 healthy |
| **Memory Usage** | 72 MB | 85 MB | Grafana: "Memory Usage" gauge<br/>Metric: `backend_memory_usage_bytes / 1024 / 1024` | <512 MB |
| **Success Rate** | 100% | 9% (3,000 success, 9,908 sold out) | Prometheus: `sum(rate(booking_attempts_total{status="success"}[1m])) / sum(rate(booking_attempts_total[1m]))` | >95% |
| **Total Requests** | 4,020 | 10,908 | k6 output: `http_reqs` | - |

### 4.2 Cara Mendapatkan Metrik dari Grafana/Prometheus

#### **A. Via Grafana Dashboard**
1. Buka http://localhost:3000
2. Login: admin/admin
3. Navigate ke dashboard: "Ticket Booking Metrics" (`/d/ticket-booking-metrics`)
4. Panels tersedia:
   - **Panel 1**: Request Rate (RPS) - Time series graph
   - **Panel 2**: P95 Response Time - Time series graph
   - **Panel 3**: Error Rate - Percentage gauge
   - **Panel 4**: Backend CPU Usage - Percentage gauge  
   - **Panel 5**: Database Latency (p95) - Time series graph
   - **Panel 6**: Tickets Remaining - Gauge
   - **Panel 7**: Concurrent Requests - Time series graph

#### **B. Via Prometheus Query Interface**
1. Buka http://localhost:9090
2. Execute queries di "Graph" tab:

```promql
# RPS
sum(rate(http_requests_total[1m]))

# P95 Latency
histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[1m]))

# Error Rate
sum(rate(http_requests_total{status=~"4..|5.."}[1m])) / sum(rate(http_requests_total[1m]))

# CPU Usage
backend_cpu_usage_percent

# DB Latency
histogram_quantile(0.95, rate(db_query_duration_seconds_bucket{operation="select"}[1m]))
```

#### **C. Via Endpoint /metrics**
```bash
# Metrik mentah - semua custom metric
curl http://localhost:8000/metrics | grep -E "backend_cpu|backend_memory|tickets_remaining|booking_attempts"

# Contoh output:
# booking_attempts_total{status="failed"} 79777.0
# booking_attempts_total{status="success"} 50.0
# tickets_remaining{event_id="1"} 56.0
# tickets_remaining{event_id="9"} 88.0
# backend_cpu_usage_percent 0.0  (idle)
# backend_cpu_usage_percent 75.0 (saat beban)
# backend_memory_usage_bytes 7.872512e+07  (~78MB)

# Metrik HTTP request
curl http://localhost:8000/metrics | grep http_requests_total
# Contoh output:
# http_requests_total{handler="/metrics",method="GET",status="200"} 400.0
# http_requests_total{handler="/book",method="POST",status="404"} 79777.0
# http_requests_total{handler="/status",method="GET",status="200"} 467.0
# http_requests_total{handler="/book",method="POST",status="200"} 50.0

# Metrik latency database
curl http://localhost:8000/metrics | grep db_query_duration
```
### 4.3 Lokasi Screenshot Bukti
- `screenshots/normal-load.png` - Grafana dashboard saat beban normal
- `screenshots/high-load.png` - Grafana dashboard saat beban tinggi  
- `screenshots/k6-output-normal.png` - k6 test results beban normal
- `screenshots/k6-output-high.png` - k6 test results beban tinggi

---

## ğŸ” 5. ANALISIS & DIAGNOSA PERFORMA

### 5.1 Akar Masalah Performa (#1): Race Condition & No Transaction Locking

#### **A. Deskripsi Masalah**
Sistem mengalami **overselling** tickets pada beban tinggi - menjual lebih banyak tiket dari yang tersedia karena tidak ada proper locking mechanism.

#### **B. Evidence dari Metrik**
- **Tickets Remaining**: Turun ke 0, tetapi masih ada booking attempts
- **Error Rate**: 45.4% pada beban tinggi (mayoritas 400 "Sold Out")
- **Success Rate**: Hanya 9% (3,000 dari 33,000 attempts)
- **Booking Attempts**: 
  - Success: 3,000
  - Sold Out: 9,908
  - Failed: 16,817 (404 errors sebelum event setup)

#### **C. Root Cause Analysis**
```python
# NAIVE IMPLEMENTATION (backend/main.py)
# Step 1: Check availability (NO LOCK!)
cursor.execute("SELECT available_tickets FROM events WHERE id = %s", (event_id,))
event = cursor.fetchone()

if event['available_tickets'] <= 0:
    raise HTTPException(status_code=400, detail="Sold out!")

# Step 2: Sleep 10ms (simulates processing - MAKES RACE CONDITION WORSE!)
time.sleep(0.01)

# Step 3: Decrement (RACE CONDITION HERE!)
cursor.execute("""
    UPDATE events 
    SET available_tickets = available_tickets - 1 
    WHERE id = %s
""", (event_id,))
```

**Race Condition Flow**:
```
Time | User A                    | User B                    | DB Tickets
-----|---------------------------|---------------------------|----------
t0   | SELECT ... â†’ 1 ticket     |                           | 1
t1   |                           | SELECT ... â†’ 1 ticket     | 1
t2   | Check: 1 > 0 âœ“           |                           | 1
t3   |                           | Check: 1 > 0 âœ“           | 1
t4   | UPDATE -1                 |                           | 0
t5   |                           | UPDATE -1                 | -1 âš ï¸
```

#### **D. Dampak pada Metrik**
- P95 Latency melonjak ke **23.8s** (dari 480ms)
- Error rate **45.4%** (seharusnya <1%)
- Success rate anjlok ke **9%** (seharusnya >95%)

#### **E. Solusi yang Disarankan**
1. **Database-level locking**:
   ```sql
   SELECT available_tickets FROM events WHERE id = ? FOR UPDATE;
   ```
2. **Optimistic locking** dengan version column
3. **Redis distributed lock** untuk horizontal scaling
4. **Queue system** (RabbitMQ/SQS) untuk rate limiting

---

### 5.2 Akar Masalah Performa (#2): No Connection Pooling

#### **A. Deskripsi Masalah**
Backend membuka **new database connection per request**, causing:
- High connection overhead (TCP handshake + auth)
- Database connection exhaustion risk
- Increased latency

#### **B. Evidence dari Metrik**
- **DB Connection Latency**: 
  - p95: 150ms pada beban tinggi
  - p50: 45ms pada beban normal
  - `db_query_duration_seconds_sum{operation="connect"}` menunjukkan ~29s total untuk 34,741 connections
- **Database Query Breakdown**:
  - Connect: 28.56s (34,741 ops)
  - Select: 5.13s (37,739 ops)
  - Insert: 0.68s (3,753 ops)
- **Connection time** = **~46% dari total DB time**

#### **C. Root Cause Analysis**
```python
# CURRENT (backend/main.py)
def get_db_connection():
    return mysql.connector.connect(
        host=os.getenv("DB_HOST", "mariadb"),
        # ... NEW CONNECTION SETIAP KALI!
    )

@app.post("/book")
async def book_ticket(request: BookRequest):
    connection = get_db_connection()  # âš ï¸ NEW CONNECTION
    cursor = connection.cursor()
    # ... do work ...
    cursor.close()
    connection.close()  # âš ï¸ CLOSE CONNECTION
```

**Impact**:
- 34,741 connections dalam 30 detik = **1,158 connections/second**
- MariaDB default `max_connections` = 151
- Connection thrashing â†’ latency spike

#### **D. Dampak pada Metrik**
- Database latency p95: **150ms** (seharusnya <50ms)
- P95 response time: **23.8s** (connection queueing)
- Backend CPU: Only **10%** (idle waiting for DB)

#### **E. Solusi yang Disarankan**
1. **Connection pooling** dengan SQLAlchemy:
   ```python
   engine = create_engine(
       "mysql+pymysql://...",
       pool_size=20,
       max_overflow=10,
       pool_pre_ping=True
   )
   ```
2. **Read replicas** untuk SELECT queries
3. **Persistent connections** (keep-alive)

---

### 5.3 Metrik Tambahan yang Diperlukan

#### **Metrik Tambahan #1: Connection Pool Metrics**

**Mengapa Diperlukan**:
- Identify connection exhaustion
- Measure pool efficiency  
- Detect connection leaks

**Metrics yang Dibutuhkan**:
```promql
# Total connections
db_connections_total{state="active|idle|waiting"}

# Pool utilization
db_connection_pool_size
db_connection_pool_used
db_connection_pool_available

# Connection wait time
db_connection_wait_duration_seconds
```

**Implementation**:
```python
from prometheus_client import Gauge

db_pool_size = Gauge('db_connection_pool_size', 'Connection pool size')
db_pool_used = Gauge('db_connection_pool_used', 'Connections in use')
db_pool_available = Gauge('db_connection_pool_available', 'Available connections')
```

**Threshold**:
- Warning: pool utilization >80%
- Critical: pool utilization >95%

---

#### **Metrik Tambahan #2: Queue Depth & Request Queueing Time**

**Mengapa Diperlukan**:
- Measure backend saturation
- Identify worker bottleneck
- Predict system capacity

**Metrics yang Dibutuhkan**:
```promql
# Queue depth
http_request_queue_depth

# Time spent waiting in queue
http_request_queue_duration_seconds

# Worker saturation
worker_utilization_percent
```

**Implementation**:
```python
from prometheus_client import Histogram, Gauge

request_queue_depth = Gauge('http_request_queue_depth', 'Requests waiting')
queue_duration = Histogram('http_request_queue_duration_seconds', 'Time in queue')

@app.middleware("http")
async def queue_tracking(request, call_next):
    queue_start = time.time()
    request_queue_depth.inc()
    response = await call_next(request)
    queue_duration.observe(time.time() - queue_start)
    request_queue_depth.dec()
    return response
```

**Threshold**:
- Warning: queue depth >50
- Critical: queue depth >200

**Analysis Value**:
- Correlate dengan CPU usage untuk horizontal scaling decisions
- Identify if bottleneck is CPU vs I/O vs locking

---

### 5.4 Incident Response Flow (P95 Latency & Error Rate Spike)

#### **ğŸš¨ INCIDENT RESPONSE PLAYBOOK**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 1: DETECTION & TRIAGE (0-5 minutes)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Alert Triggered:
âœ“ Grafana Alert: "P95 Latency > 3s for 5 minutes"
âœ“ Grafana Alert: "Error Rate > 5% for 2 minutes"

Immediate Actions:
1. Check Grafana dashboard: http://localhost:3000/d/ticket-booking-metrics
2. Verify alert is real (not false positive)
3. Assess impact:
   - Check RPS: Is traffic spiking?
   - Check Error Rate: What % of users affected?
   - Check Tickets Remaining: Business impact?
4. Open incident ticket: "P1 - High Latency & Errors"
5. Notify on-call team via PagerDuty/Slack

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 2: IMMEDIATE MITIGATION (5-15 minutes)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Quick Checks:
â–¡ Backend healthy? â†’ curl http://localhost:8000/health
â–¡ Database healthy? â†’ docker exec ticket-mariadb mysqladmin ping
â–¡ Disk space? â†’ df -h
â–¡ Memory? â†’ docker stats

Mitigation Options (choose based on symptoms):

IF CPU > 90%:
  â†’ Scale horizontally: docker-compose up -d --scale backend=3
  
IF Memory > 90%:
  â†’ Restart backend: docker-compose restart backend
  â†’ Check for memory leak: docker stats --no-stream
  
IF DB Latency > 500ms:
  â†’ Check slow queries: SHOW PROCESSLIST;
  â†’ Kill long-running queries: KILL <query_id>;
  â†’ Restart database (last resort): docker-compose restart mariadb
  
IF Error Rate > 10%:
  â†’ Check logs: docker logs ticket-backend --tail 100
  â†’ Identify error pattern (404, 400, 500?)
  â†’ Rollback recent deployment if needed

Temporary Traffic Control:
â†’ Enable rate limiting at nginx level
â†’ Return 503 "Service Unavailable" for non-critical endpoints
â†’ Prioritize /book endpoint over /status

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 3: ROOT CAUSE ANALYSIS (15-60 minutes)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Prometheus Queries for RCA:

1. Identify traffic pattern:
   sum by(handler) (rate(http_requests_total[5m]))
   
2. Check for specific endpoint issues:
   histogram_quantile(0.95, sum by(handler, le) 
     (rate(http_request_duration_seconds_bucket[5m])))
   
3. Database operation breakdown:
   sum by(operation, table) (rate(db_query_duration_seconds_count[5m]))
   
4. Error type distribution:
   sum by(status) (rate(http_requests_total{status=~"4..|5.."}[5m]))
   
5. Concurrent request pattern:
   max_over_time(http_requests_inprogress{handler="/book"}[5m])

Correlation Analysis:
- High RPS â†’ High CPU â†’ High Latency? â†’ Need horizontal scaling
- High RPS â†’ Low CPU â†’ High Latency? â†’ Database bottleneck
- High Concurrent Requests â†’ High Latency? â†’ Locking contention
- Growing Memory â†’ High GC time? â†’ Memory leak

Check Application Logs:
docker logs ticket-backend --since 30m | grep -E "ERROR|Exception"

Check Database Metrics:
SHOW STATUS LIKE 'Threads_connected';
SHOW STATUS LIKE 'Slow_queries';
SHOW GLOBAL STATUS LIKE 'Questions';

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 4: PERMANENT FIX (1-48 hours)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Based on RCA, implement fixes:

Race Condition Fix:
â†’ Add database-level locking (FOR UPDATE)
â†’ Implement optimistic locking
â†’ Add integration tests

Connection Pool Fix:
â†’ Implement SQLAlchemy with connection pooling
â†’ Configure pool_size=20, max_overflow=10
â†’ Monitor new metrics: db_connection_pool_utilization

Scale Infrastructure:
â†’ Horizontal: Add backend replicas
â†’ Vertical: Increase CPU/memory limits
â†’ Database: Add read replicas

Code Optimization:
â†’ Add caching layer (Redis) for /status endpoint
â†’ Reduce time.sleep(0.01) in booking logic
â†’ Async database queries

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 5: POST-MORTEM (24-72 hours)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Document incident:
1. Timeline of events
2. Impact metrics:
   - Duration: X minutes
   - Affected users: Y requests failed
   - Revenue impact: Z bookings lost
3. Root cause: [Race condition / Connection exhaustion / ...]
4. Actions taken
5. Lessons learned
6. Follow-up tasks

Preventive Measures:
â–¡ Add alerting for leading indicators
â–¡ Improve load testing scenarios
â–¡ Add chaos engineering tests
â–¡ Update runbooks
â–¡ Conduct training session
```

#### **Key Metrics to Monitor During Incident**

| Phase | Primary Metrics | Secondary Metrics |
|-------|----------------|-------------------|
| **Detection** | P95 Latency, Error Rate | RPS, Concurrent Requests |
| **Mitigation** | Error Rate, Success Rate | CPU, Memory |
| **RCA** | All 8 core metrics | DB connections, Queue depth |
| **Validation** | P95 Latency, Error Rate | Tickets remaining, Success rate |

---

## ğŸ’¡ 6. EVALUASI & IMPROVEMENT PLAN

### 6.1 Keputusan Arsitektur yang Tepat

#### **Keputusan #1: Containerization dengan Docker Compose**

**Mengapa Tepat**:
- âœ… **Portable**: Deploy di Windows/Linux/Mac tanpa masalah
- âœ… **Reproducible**: `docker-compose up` instant environment
- âœ… **Isolated**: Setiap service punya network & resources sendiri
- âœ… **Scalable**: Mudah scale dengan `--scale backend=3`
- âœ… **Version controlled**: Infrastructure as Code (docker-compose.yml)

**Evidence**:
- Deploy time: <2 minutes dari scratch
- Zero dependency conflicts
- Easy rollback: `docker-compose down && docker-compose up`

**Best Practice yang Diikuti**:
- Health checks untuk setiap service
- Explicit dependency management (depends_on)
- Volume mounting untuk persistent data
- Environment variable management

---

#### **Keputusan #2: Prometheus + Grafana untuk Observability**

**Mengapa Tepat**:
- âœ… **Industry Standard**: Prometheus adalah de-facto untuk metrics
- âœ… **Pull-based**: Backend tidak perlu tahu tentang monitoring
- âœ… **PromQL**: Powerful query language untuk complex analysis
- âœ… **Low Overhead**: <1% CPU impact pada backend
- âœ… **Time-series**: Historical analysis & trend detection

**Evidence**:
- 8 key metrics implemented dengan mudah
- 5-second scrape interval tanpa bottleneck
- Grafana dashboard ready dalam <5 menit
- Query response time <100ms

**Best Practice yang Diikuti**:
- RED metrics (Rate, Errors, Duration) implemented
- USE metrics (Utilization, Saturation, Errors) for resources
- Business metrics (tickets_remaining) alongside tech metrics
- Histogram untuk latency distribution (bukan hanya average)

---

### 6.2 Improvement Jangka Menengah/Panjang

#### **Improvement #1: Database Connection Pooling**

**Implementation**:
```python
# backend/main.py
from sqlalchemy import create_engine
from sqlalchemy.pool import QueuePool

engine = create_engine(
    "mysql+pymysql://ticket_user:ticket_pass@mariadb:3306/ticket_booking",
    poolclass=QueuePool,
    pool_size=20,          # Base connections
    max_overflow=10,       # Extra connections on spike
    pool_timeout=30,       # Wait time for connection
    pool_recycle=3600,     # Reconnect after 1 hour
    pool_pre_ping=True,    # Validate connection before use
    echo=False
)
```

**Dampak**:

| Aspek | Sebelum | Sesudah | Improvement |
|-------|---------|---------|-------------|
| **Performa** | DB connect: 28.5s (34,741 ops) | DB connect: ~0.1s (reuse) | **285x faster** |
| **Latency** | p95: 23.8s | p95: ~500ms | **47x reduction** |
| **Throughput** | 361 RPS | ~1,200 RPS | **3.3x increase** |
| **Reliability** | Connection exhaustion risk | Stable connection pool | **Highly reliable** |
| **Cost** | Need more DB instances | 1 instance sufficient | **-60% DB cost** |
| **Sustainability** | High connection churn â†’ CPU waste | Efficient reuse | **-80% connection overhead** |

**Metrik Keberhasilan**:
```promql
# 1. Connection pool utilization < 80%
db_connection_pool_used / db_connection_pool_size < 0.8

# 2. Connection wait time < 10ms
histogram_quantile(0.95, rate(db_connection_wait_duration_seconds_bucket[1m])) < 0.01

# 3. DB connect latency < 5ms
histogram_quantile(0.95, rate(db_query_duration_seconds_bucket{operation="connect"}[1m])) < 0.005

# 4. Throughput increase
rate(http_requests_total[1m]) > 1000
```

**Timeline**: 1-2 weeks
**Effort**: Medium (code refactor + testing)
**Risk**: Low (backward compatible)

---

#### **Improvement #2: Redis Caching Layer**

**Implementation**:
```python
# backend/main.py
import redis
from functools import wraps

redis_client = redis.Redis(host='redis', port=6379, decode_responses=True)

def cache_result(ttl=60):
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            cache_key = f"{func.__name__}:{args}:{kwargs}"
            cached = redis_client.get(cache_key)
            if cached:
                return json.loads(cached)
            result = await func(*args, **kwargs)
            redis_client.setex(cache_key, ttl, json.dumps(result))
            return result
        return wrapper
    return decorator

@app.get("/status")
@cache_result(ttl=5)  # Cache 5 seconds
async def get_status():
    # Expensive DB query avoided 90% of the time
    return fetch_from_database()
```

**Architecture Update**:
```
Backend â”€â”€â”¬â”€â”€â–¶ Redis (Cache) â”€â”€cache missâ”€â”€â–¶ MariaDB
          â”‚                                      â”‚
          â””â”€â”€â”€â”€â”€â”€cache hit (90%)â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Dampak**:

| Aspek | Sebelum | Sesudah | Improvement |
|-------|---------|---------|-------------|
| **Performa** | /status: 150ms (DB query) | /status: 2ms (Redis) | **75x faster** |
| **Database Load** | 100% queries hit DB | 10% queries hit DB | **-90% DB load** |
| **Cost** | Need bigger DB instance | Small DB + Redis | **-40% total cost** |
| **Reliability** | DB failure = total outage | Cached data survives | **Cache failover** |
| **Sustainability** | High DB CPU/IO | Low DB utilization | **-70% energy** |
| **Scalability** | DB bottleneck at 1K RPS | Handle 10K RPS | **10x capacity** |

**Cache Strategy**:
```python
# Hot data: 5s TTL
- GET /status (ticket availability)
- GET /events (event list)

# Warm data: 60s TTL  
- GET /stats (booking statistics)

# Cold data: 5min TTL
- GET /history (booking history)

# Never cache:
- POST /book (write operation)
```

**Metrik Keberhasilan**:
```promql
# 1. Cache hit rate > 80%
cache_hits_total / (cache_hits_total + cache_misses_total) > 0.8

# 2. /status latency < 10ms
histogram_quantile(0.95, rate(http_request_duration_seconds_bucket{handler="/status"}[1m])) < 0.01

# 3. Database query reduction
rate(db_query_duration_seconds_count{operation="select"}[1m]) < 100

# 4. RPS increase capability
rate(http_requests_total[1m]) > 5000
```

**Timeline**: 2-3 weeks
**Effort**: Medium (new service + cache invalidation logic)
**Risk**: Medium (cache consistency, invalidation bugs)

---

#### **Improvement #3: Horizontal Auto-scaling dengan Kubernetes**

**Implementation**:
```yaml
# kubernetes/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ticket-backend
spec:
  replicas: 3  # Base replicas
  selector:
    matchLabels:
      app: ticket-backend
  template:
    spec:
      containers:
      - name: backend
        image: ticket-backend:latest
        resources:
          requests:
            cpu: 500m
            memory: 256Mi
          limits:
            cpu: 1000m
            memory: 512Mi

---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: ticket-backend-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: ticket-backend
  minReplicas: 3
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70  # Scale when CPU > 70%
  - type: Pods
    pods:
      metric:
        name: http_requests_per_second
      target:
        type: AverageValue
        averageValue: "500"  # Scale when RPS > 500 per pod
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 50  # Scale up 50% at a time
        periodSeconds: 60
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Pods
        value: 1  # Scale down 1 pod at a time
        periodSeconds: 180
```

**Architecture Update**:
```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Load Balancer  â”‚
                    â”‚   (Ingress)     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚                 â”‚                 â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
    â”‚  Backend    â”‚  â”‚  Backend    â”‚  â”‚  Backend    â”‚
    â”‚   Pod 1     â”‚  â”‚   Pod 2     â”‚  â”‚   Pod 3-20  â”‚
    â”‚  (Always)   â”‚  â”‚  (Always)   â”‚  â”‚  (Auto)     â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
           â”‚                 â”‚                 â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                      â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
                      â”‚   MariaDB   â”‚
                      â”‚  (StatefulSet)
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Dampak**:

| Aspek | Sebelum (Docker Compose) | Sesudah (K8s + HPA) | Improvement |
|-------|--------------------------|---------------------|-------------|
| **Performa** | 1 backend pod (max 600 RPS) | 3-20 pods (max 12K RPS) | **20x peak capacity** |
| **Reliability** | Single point of failure | Multi-pod redundancy | **99.9% uptime** |
| **Cost** | Always 3 pods running | Scale down to 3 (off-peak) | **-40% cost off-peak** |
| **Latency** | p95: 500ms (normal) | p95: 300ms (distributed load) | **40% faster** |
| **Sustainability** | Idle resources 60% of time | Match load (utilization 70%) | **-50% waste** |
| **Scalability** | Manual scaling | Auto-scale in 60s | **Instant response** |

**Scaling Scenarios**:

| Time | RPS | CPU | Pods | Action |
|------|-----|-----|------|--------|
| 00:00 (Midnight) | 50 | 10% | 3 | Minimum replicas |
| 09:00 (Morning) | 300 | 60% | 3 | Stable |
| 12:00 (Presale Start) | 5,000 | 95% | 10 | **Auto-scale up** |
| 12:05 (Peak) | 10,000 | 85% | 20 | **At maximum** |
| 12:30 (After Peak) | 1,000 | 40% | 6 | **Scaling down** |
| 18:00 (Evening) | 200 | 30% | 3 | Back to minimum |

**Metrik Keberhasilan**:
```promql
# 1. Auto-scaling responsiveness < 60s
time_to_scale_up_seconds < 60

# 2. CPU utilization 60-80% (efficient)
avg(container_cpu_usage_percent) > 0.6 AND avg(container_cpu_usage_percent) < 0.8

# 3. No failed requests during scale events
rate(http_requests_total{status="503"}[1m]) during scaling == 0

# 4. Cost efficiency (pod-hours vs RPS)
sum(kube_pod_container_resource_requests{container="backend"}) / sum(rate(http_requests_total[1h])) decreasing

# 5. Latency stability during scaling
stddev_over_time(http_request_duration_seconds{quantile="0.95"}[5m]) < 0.5
```

**Timeline**: 4-8 weeks
**Effort**: High (K8s learning curve + migration)
**Risk**: Medium-High (complexity, cost of K8s cluster)

**Additional Benefits**:
- **Rolling updates**: Zero-downtime deployments
- **Health checks**: Auto-restart unhealthy pods
- **Resource quotas**: Prevent resource exhaustion
- **Monitoring**: Built-in Prometheus integration
- **Multi-cloud**: Portability (GKE, EKS, AKS)

---

## ğŸ“ Project Structure
â”œâ”€â”€ docker-compose.yml      # Orchestration file
â””â”€â”€ README.md              # This file
```

## ğŸš€ Mulai Cepat

### Prasyarat
- Docker & Docker Compose sudah terinstal
- Port tersedia: 8000, 8080, 3000, 9090, 3306

### 1. Jalankan Semua Service

```bash
docker-compose up -d
```

Tunggu semua service sehat (sekitar 30 detik):

```bash
docker-compose ps
```

### 2. Setup Event

Inisialisasi event konser dengan 100 tiket:

**Opsi A: Via Frontend**
- Buka http://localhost:8080
- Klik tombol "Reset Event (100 tickets)"

**Opsi B: Via API**
```bash
curl -X POST http://localhost:8000/setup \
  -H "Content-Type: application/json" \
  -d '{"event_name": "Coldplay Concert - Jakarta", "total_tickets": 100}'
```

### 3. Akses Aplikasi

| Service | URL | Kredensial |
|---------|-----|------------|
| **Frontend** | http://localhost:8080 | Tidak ada |
| **Backend API** | http://localhost:8000/docs | Tidak ada |
| **Prometheus** | http://localhost:9090 | Tidak ada |
| **Grafana** | http://localhost:3000 | admin/admin |

## ğŸ« API Endpoints

### POST /setup
Reset dan inisialisasi event dengan tiket
```json
{
  "event_name": "Coldplay Concert - Jakarta",
  "total_tickets": 100
}
```

### POST /book
Memesan tiket (implementasi NAIF - mendemonstrasikan race condition)
```json
{
  "user_id": 1001,
  "event_id": 1
}
```

### GET /status
Mendapatkan ketersediaan tiket saat ini
```bash
curl http://localhost:8000/status
```

### GET /metrics
Endpoint metrik Prometheus
```bash
curl http://localhost:8000/metrics
```

## ğŸ”¥ Demonstrasi Race Condition

### Masalah
Endpoint `/book` sengaja diimplementasikan TANPA locking atau transaksi yang tepat. Ini berarti:
1. Beberapa request dapat membaca jumlah tiket yang sama secara bersamaan
2. Semua dapat lolos pengecekan "tiket tersedia"
3. Semua akan mengurangi jumlah, menyebabkan **overselling**

### Langkah untuk Mereproduksi

1. **Setup Event**
   ```bash
   curl -X POST http://localhost:8000/setup \
     -H "Content-Type: application/json" \
     -d '{"total_tickets": 100}'
   ```

2. **Run Load Test**
   ```bash
   docker exec -it ticket-k6 k6 run /scripts/load-test.js
   ```

3. **Check Results**
   ```bash
   curl http://localhost:8000/status
   ```

4. **Hasil yang Diharapkan**: 
   - Lebih dari 100 booking tercatat di database
   - Tiket tersedia negatif atau nol
   - Mendemonstrasikan race condition!

## ğŸ“Š Observabilitas

### Metrik Komprehensif Tersedia

Aplikasi mengekspos metrik detail untuk monitoring:

1. **ğŸ“ˆ Request Per Second (RPS)** - `rate(http_requests_total[1m])`
2. **â±ï¸ P95 Response Time** - `histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (le))`
3. **âŒ Error Rate (4xx/5xx)** - `sum(rate(http_requests_total{status=~"4..|5.."}[1m])) / sum(rate(http_requests_total[1m])) * 100`
4. **ğŸ–¥ï¸ Backend CPU Usage** - `backend_cpu_usage_percent`
5. **ğŸ’¾ Database Latency** - `histogram_quantile(0.95, sum(rate(db_query_duration_seconds_bucket[5m])) by (le, operation))`
6. **ğŸ« Tickets Remaining** - `tickets_remaining`
7. **ğŸ“‹ Booking Success/Failure** - `booking_attempts_total`
8. **ğŸ’¾ Memory Usage** - `backend_memory_usage_bytes`
9. **ğŸ”„ Concurrent Requests** - `http_requests_inprogress`
10. **ğŸ”Œ DB Connection Errors** - `db_connection_errors_total`

### Tes Metrik Cepat

Jalankan script tes untuk menghasilkan traffic sampel dan verifikasi metrik:

```powershell
.\test-metrics.ps1
```

Ini akan:
- Setup event
- Membuat booking sampel
- Menampilkan status saat ini
- Menampilkan semua URL metrik

### Metrik Prometheus

Akses Prometheus di http://localhost:9090 dan query:

```promql
# Request rate
rate(http_requests_total[1m])

# Error rate
rate(http_requests_total{status=~"5.."}[1m])

# Request duration (p95)
histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))

# Total bookings per minute
rate(http_requests_total{method="POST", handler="/book"}[1m])
```

### Dashboard Grafana

1. Login ke Grafana: http://localhost:3000 (admin/admin)
2. Datasource Prometheus sudah terkonfigurasi
3. **Dashboard pre-built otomatis dimuat**: "Concert Ticket Booking - Comprehensive Metrics"
4. Atau buat panel kustom dengan query berikut:
   - **Request Rate**: `sum(rate(http_requests_total[1m])) by (handler)`
   - **Error Rate**: `sum(rate(http_requests_total{status=~"4..|5.."}[1m])) / sum(rate(http_requests_total[1m])) * 100`
   - **Response Time (p95)**: `histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (le, handler))`
   - **CPU Usage**: `backend_cpu_usage_percent`
   - **Database Latency**: `histogram_quantile(0.95, sum(rate(db_query_duration_seconds_bucket[5m])) by (le, operation))`

**ğŸ“– For detailed metrics guide, see [METRICS_GUIDE.md](METRICS_GUIDE.md)**

## ğŸ§ª Load Testing dengan k6

### Tes Dasar
```bash
docker exec -it ticket-k6 k6 run /scripts/load-test.js
```

### Tes Kustom (100 pengguna bersamaan)
```bash
docker exec -it ticket-k6 k6 run --vus 100 --duration 30s /scripts/load-test.js
```

### Tes Agresif (untuk memicu race condition)
```bash
docker exec -it ticket-k6 k6 run --vus 200 --duration 10s /scripts/load-test.js
```

## ğŸ”§ Pemecahan Masalah

### Service tidak berjalan
```bash
# Cek log
docker-compose logs backend
docker-compose logs mariadb

# Restart service
docker-compose restart
```

### Masalah koneksi database
```bash
# Cek apakah MariaDB siap
docker exec -it ticket-mariadb mysql -uroot -proot -e "SHOW DATABASES;"
```

### Reset semuanya
```bash
docker-compose down -v
docker-compose up -d
```

## ğŸ“ Untuk Demo Proyek Universitas

### Poin Utama yang Perlu Disorot:

1. **Demo Race Condition**
   - Tunjukkan implementasi naif dalam kode
   - Jalankan load test
   - Buktikan overselling terjadi
   - Jelaskan cara memperbaiki (transaksi, lock, antrian)

2. **Observabilitas**
   - Tunjukkan metrik Prometheus
   - Tampilkan dashboard Grafana
   - Jelaskan metodologi RED

3. **Arsitektur Cloud Native**
   - Service terpisah (frontend, backend, database)
   - Dikontainerisasi dengan Docker
   - Diorkestrasi dengan Docker Compose
   - Desain aplikasi stateless

4. **Analisis Bottleneck**
   - Gunakan Prometheus untuk mengidentifikasi endpoint lambat
   - Tunjukkan performa query database
   - Demonstrasikan dimana lock diperlukan

## ğŸ› ï¸ Memperbaiki Race Condition (Untuk Diskusi)

Implementasi saat ini bersifat naif. Berikut solusinya:

### Opsi 1: Transaksi Database dengan Row Locking
```python
cursor.execute("""
    SELECT available_tickets FROM events 
    WHERE id = %s FOR UPDATE
""", (event_id,))
```

### Opsi 2: Update Atomik
```python
cursor.execute("""
    UPDATE events 
    SET available_tickets = available_tickets - 1 
    WHERE id = %s AND available_tickets > 0
""", (event_id,))
if cursor.rowcount == 0:
    raise HTTPException(400, "Sold out!")
```

### Opsi 3: Message Queue
- Gunakan Redis/RabbitMQ untuk serialisasi request booking
- Menjamin urutan dan mencegah race condition

## ğŸ“ Lisensi

Ini adalah proyek universitas untuk tujuan pendidikan.

## ğŸ‘¨â€ğŸ’» Penulis

Dibuat untuk demonstrasi mata kuliah Arsitektur Cloud Native.

---

**Catatan**: Sistem ini sengaja mengandung race condition untuk tujuan pendidikan. JANGAN gunakan di production tanpa mekanisme sinkronisasi yang tepat!

---

##  7. STRUKTUR PROYEK & DEPLOYMENT

### 7.1 Struktur Direktori

```
cloudservices/
 backend/
    main.py                    # FastAPI application (363 lines)
    requirements.txt           # Python dependencies
    Dockerfile                 # Backend container image
 frontend/
    index.html                 # Bootstrap 5 UI
    nginx.conf                 # Nginx reverse proxy config
 prometheus/
    prometheus.yml             # Scrape config (5s interval)
 grafana/
    provisioning/
       datasources/
           prometheus.yml     # Auto datasource config
    dashboards/
        ticket-booking-dashboard.json  # Pre-built dashboard
 k6/
    load-test.js               # Load testing scenarios
 docker-compose.yml             # Orchestration configuration
 create-dashboard.ps1           # Dashboard provisioning script
 test-metrics.ps1               # Quick test script
 README.md                      # This comprehensive documentation
```

### 7.2 Quick Start Guide

#### **A. Prerequisites**
```powershell
# Check requirements
docker --version          # Docker 20.10+
docker-compose --version  # Compose 2.0+
curl --version           # For API testing
```

#### **B. Deploy Full Stack**
```powershell
# 1. Clone & navigasi
cd G:\App\cloudservices

# 2. Jalankan semua service (build + run)
docker-compose up -d --build

# 3. Tunggu health check (30 detik)
Start-Sleep -Seconds 30

# 4. Verifikasi semua service berjalan
docker-compose ps

# Output yang diharapkan:
# NAME               STATUS              PORTS
# ticket-backend     Up (healthy)        0.0.0.0:8000->8000/tcp
# ticket-frontend    Up                  0.0.0.0:80->80/tcp
# ticket-mariadb     Up (healthy)        3306/tcp
# ticket-prometheus  Up                  0.0.0.0:9090->9090/tcp
# ticket-grafana     Up                  0.0.0.0:3000->3000/tcp
# ticket-k6          Up                  (on-demand)
```

#### **C. Inisialisasi Event**
```powershell
# Setup event konser dengan 5000 tiket
$body = @{
    event_name = "Concert Metallica Jakarta"
    total_tickets = 5000
} | ConvertTo-Json

curl.exe -X POST http://localhost:8000/setup `
  -H "Content-Type: application/json" `
  -d $body

# Response:
# {"message":"Event setup successful","event_id":1,"event_name":"Concert Metallica Jakarta","total_tickets":5000}
```

#### **D. Buat Dashboard Grafana**
```powershell
# Otomatis buat dashboard via API
.\create-dashboard.ps1

# Output:
# Dashboard URL: http://localhost:3000/d/ticket-booking-metrics
```

#### **E. Jalankan Load Test**
```powershell
# Beban Normal (50 VUs)
docker exec ticket-k6 k6 run --vus 50 --duration 60s /scripts/load-test.js

# Beban Tinggi (300 VUs)
docker exec ticket-k6 k6 run --vus 300 --duration 30s /scripts/load-test.js
```

#### **F. Akses Monitoring**
```
Frontend:    http://localhost          (UI Booking)
Backend API: http://localhost:8000     (FastAPI docs di /docs)
Prometheus:  http://localhost:9090     (Metrik & query)
Grafana:     http://localhost:3000     (Dashboard: admin/admin)
             > /d/ticket-booking-metrics
```

---

##  8. KESIMPULAN & HASIL PEMBELAJARAN

### 8.1 Pencapaian Proyek

 **Arsitektur Cloud-Native Lengkap**:
- Frontend (Nginx), Backend (FastAPI), Database (MariaDB)
- Containerization penuh dengan Docker Compose
- Isolasi service dengan Docker network
- Health check & manajemen dependensi

 **Stack Observabilitas Siap Production**:
- Metrik Prometheus (8 metrik inti)
- Dashboard Grafana (7 panel visualisasi)
- Instrumentasi kustom (CPU, memori, metrik bisnis)
- Interval scrape 5 detik, retensi 15 hari

 **Load Testing Komprehensif**:
- 2 skenario realistis (beban normal & tinggi)
- 300 pengguna bersamaan diuji
- 10,908 total request dalam 30 detik
- Bottleneck performa teridentifikasi

 **Analisis Performa Mendalam**:
- 2 akar masalah teridentifikasi (race condition, tidak ada connection pooling)
- Dampak terkuantifikasi dengan metrik
- 2 metrik tambahan diusulkan
- Playbook respons insiden lengkap

 **Rencana Improvement Dapat Ditindaklanjuti**:
- 3 improvement dengan estimasi timeline & effort
- Dampak pada performa/reliabilitas/biaya/keberlanjutan
- Metrik keberhasilan untuk setiap improvement
- Roadmap migrasi Kubernetes

### 8.2 Insight Utama

#### **Pembelajaran Teknis**:
1. **Connection pooling** dapat meningkatkan throughput **3.3x**
2. **Redis caching** mengurangi beban database **90%**
3. **Race condition** menyebabkan **45% error rate** pada beban tinggi
4. **P95 latency** lebih bermakna dari rata-rata (23.8s vs 4.1s)
5. **Horizontal scaling** dengan K8s dapat menangani **20x traffic**
6. **Monitoring CPU** di container memerlukan perhitungan `cpu_times()`, bukan `cpu_percent()` (mencapai akurasi 75-86% saat beban)

### 8.3 Checklist Kesiapan Production

Status Saat Ini: **60% Siap Production**

 **Selesai**:
- [x] Containerization
- [x] Orkestrasi service
- [x] Instrumentasi metrik (8 metrik inti + booking_attempts)
- [x] Pembuatan dashboard (Grafana dengan 12 panel)
- [x] Health check
- [x] Load testing
- [x] Monitoring CPU dengan background thread (`cpu_times()`)
- [x] Dukungan event_id dinamis di frontend

 **Perlu Dikerjakan**:
- [ ] Connection pooling (prioritas tinggi)
- [ ] Transaksi database dengan locking
- [ ] Penanganan error & logika retry
- [ ] Rate limiting
- [ ] Autentikasi/otorisasi API

---

**Terakhir Diperbarui**: 21 Januari 2026  
**Versi**: 1.1.0  
**Status**:  Terdokumentasi Lengkap & Teruji  
**Pembaruan Terkini**:
- Implementasi monitoring CPU diperbarui menggunakan background thread dengan `cpu_times()` untuk metrik container yang akurat
- Frontend sekarang mengambil `event_id` secara dinamis dari endpoint `/status`
- Dashboard Grafana dibuat via REST API dengan datasource UID yang benar
